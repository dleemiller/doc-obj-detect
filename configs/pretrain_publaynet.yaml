# Pretraining configuration for PubLayNet dataset

model:
  backbone: "timm/vit_pe_spatial_gigantic_patch14_448.fb"  # ViT-Gigantic (1.8B params) for maximum quality
  num_classes: 5  # PubLayNet: text, title, list, table, figure
  use_pretrained_backbone: true
  freeze_backbone: true

detr:
  # Deformable DETR architecture parameters
  num_queries: 300  # Number of object queries
  encoder_layers: 6
  decoder_layers: 6
  encoder_attention_heads: 8
  decoder_attention_heads: 8
  encoder_ffn_dim: 2048
  decoder_ffn_dim: 2048
  dropout: 0.1
  activation_function: "relu"
  auxiliary_loss: true
  num_feature_levels: 4  # Multi-scale deformable attention
  decoder_n_points: 4
  encoder_n_points: 4

data:
  dataset: "publaynet"
  train_split: "train"
  val_split: "validation"  # PubLayNet uses 'validation' not 'val'
  image_size: 448  # ViT-Gigantic expects 448x448 inputs
  batch_size: 64  # Tested to fit in 96GB VRAM (~65-70GB peak)
  num_workers: 4  # Reduced from 8 to avoid "too many open files" error

augmentation:
  horizontal_flip: 0.5
  rotate_limit: 5  # Small rotations to preserve text readability
  brightness_contrast: 0.2
  noise_std: 0.01

training:
  # TrainingArguments parameters - passed as **kwargs
  num_train_epochs: 12  # PubLayNet standard: 6-12 epochs with pretrained backbones
  learning_rate: 5.0e-5
  weight_decay: 1.0e-4
  warmup_steps: 2000
  gradient_accumulation_steps: 1
  bf16: true
  save_steps: 500  # Save more frequently with fewer total steps
  eval_steps: 500
  logging_steps: 10
  eval_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  push_to_hub: false
  # Early stopping to prevent overtraining
  early_stopping_patience: 5  # Stop if no improvement for 5 eval cycles (2500 steps)

output:
  output_dir: "outputs/pretrain_publaynet"
  checkpoint_dir: "outputs/pretrain_publaynet/checkpoints"
  log_dir: "outputs/pretrain_publaynet/logs"
