# D-FINE Pretraining configuration for PubLayNet dataset
# ConvNeXt-Large-DINOv3 backbone + D-FINE detector head

model:
  backbone: "convnext_large.dinov3_lvd1689m"  # ConvNeXt-Large DINOv3 (196M params)
  num_classes: 5  # PubLayNet: text, title, list, table, figure
  use_pretrained_backbone: true
  freeze_backbone: false  # Train end-to-end from start with split LR

dfine:
  # Backbone feature extraction (ConvNeXt stages 1, 2, 3)
  encoder_in_channels: [384, 768, 1536]  # ConvNeXt-L channel dimensions
  feat_strides: [8, 16, 32]  # Feature pyramid strides
  num_feature_levels: 3  # Multi-scale levels
  backbone_kwargs:
    out_indices: [1, 2, 3]  # Extract from stages 1, 2, 3

  # Encoder configuration
  encoder_hidden_dim: 256
  encoder_layers: 1
  encoder_ffn_dim: 1024
  encoder_attention_heads: 8

  # Decoder configuration
  d_model: 256
  num_queries: 300  # Number of object queries
  decoder_layers: 6
  decoder_ffn_dim: 1024
  decoder_attention_heads: 8
  decoder_n_points: 4

  # D-FINE specific losses (FDR + GO-LSD)
  weight_loss_vfl: 1.0  # Varifocal loss
  weight_loss_bbox: 5.0  # L1 bbox loss
  weight_loss_giou: 2.0  # GIoU loss
  weight_loss_fgl: 0.15  # Fine-grained localization
  weight_loss_ddf: 1.5  # Distribution distillation

  # Training features
  num_denoising: 100  # Denoising queries for training stability
  auxiliary_loss: true  # Deep supervision

data:
  dataset: "publaynet"
  train_split: "train"
  val_split: "validation"  # PubLayNet uses 'validation' not 'val'
  image_size: 512  # Standard document detection size
  batch_size: 40  # Tested: max=56, using 40 for stability (82.46 GB peak)
  num_workers: 4  # Reduced to avoid "too many open files"

augmentation:
  horizontal_flip: 0.5
  rotate_limit: 5  # Small rotations to preserve text readability
  brightness_contrast: 0.2
  noise_std: 0.01

training:
  # Training schedule (24 epochs for PubLayNet with pretrained backbone)
  num_train_epochs: 24

  # Learning rates (split LR: backbone 1/10 of head)
  # Base LR scaled for batch size 64
  # lr_head = 2e-4 * (64/16) = 8e-4
  # lr_backbone = 8e-4 * 0.1 = 8e-5
  learning_rate: 8.0e-4  # Head LR (backbone will be 1/10)

  # Optimizer settings
  weight_decay: 0.05  # AdamW weight decay

  # Warmup and scheduling
  warmup_steps: 1000  # Linear warmup (5% of ~20k total steps)

  # Gradient settings
  gradient_accumulation_steps: 1
  max_grad_norm: 0.5  # Gradient clipping for stability

  # Mixed precision
  bf16: true

  # Checkpointing and evaluation
  save_steps: 500
  eval_steps: 500
  logging_steps: 10
  eval_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  push_to_hub: false

output:
  output_dir: "outputs/pretrain_publaynet_dfine"
  checkpoint_dir: "outputs/pretrain_publaynet_dfine/checkpoints"
  log_dir: "outputs/pretrain_publaynet_dfine/logs"
