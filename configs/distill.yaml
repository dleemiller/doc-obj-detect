# Knowledge distillation configuration

teacher:
  checkpoint: "outputs/finetune_doclaynet/final_model"
  backbone: "convnext_large.dinov3_lvd1689m"
  detector: "dfine"

model:
  backbone: "convnext_base.dinov2"  # student backbone variant
  architecture: dfine_large  # Student architecture
  num_classes: 11
  use_pretrained_backbone: true
  freeze_backbone: false

dfine:
  encoder_in_channels: [192, 384, 768]
  feat_strides: [8, 16, 32]
  num_feature_levels: 3
  backbone_kwargs:
    out_indices: [1, 2, 3]
  encoder_hidden_dim: 256
  encoder_layers: 1
  encoder_ffn_dim: 1024
  encoder_attention_heads: 8
  d_model: 256
  num_queries: 300
  decoder_layers: 6
  decoder_ffn_dim: 1024
  decoder_attention_heads: 8
  decoder_n_points: 4
  weight_loss_vfl: 1.0
  weight_loss_bbox: 5.0
  weight_loss_giou: 2.0
  weight_loss_fgl: 0.15
  weight_loss_ddf: 1.5
  num_denoising: 100
  auxiliary_loss: true

distillation:
  loss_type: "kl"  # "kl" or "mse"
  temperature: 3.0  # Temperature for KL divergence
  alpha: 0.7  # Weight for distillation loss
  beta: 0.3  # Weight for ground truth loss (alpha + beta = 1.0)
  distill_features: true  # Also distill intermediate features

data:
  dataset: "doclaynet"
  train_split: "train"
  val_split: "val"
  image_size: 512
  batch_size: 16  # Can use larger batch for smaller student
  num_workers: 8

augmentation:
  multi_scale_sizes: [480, 512, 544, 576, 608, 640]
  max_long_side: 928
  rotate_limit: 5
  rotate_prob: 0.5
  perspective:
    probability: 0.3
    scale_min: 0.02
    scale_max: 0.05
  elastic:
    probability: 0.2
    alpha: 30
    sigma: 5
  brightness_contrast:
    limit: 0.2
    probability: 0.5
  blur:
    probability: 0.3
    blur_limit: 3
  compression:
    probability: 0.3
    quality_min: 75
    quality_max: 100
  noise:
    probability: 0.2
    std_min: 0.0
    std_max: 0.01

training:
  # TrainingArguments parameters - passed as **kwargs
  num_train_epochs: 30
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  warmup_steps: 1000
  gradient_accumulation_steps: 1
  bf16: true
  save_steps: 500
  eval_steps: 500
  logging_steps: 100
  eval_strategy: "steps"
  save_strategy: "steps"
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  push_to_hub: false

output:
  output_dir: "outputs/distill"
  checkpoint_dir: "outputs/distill/checkpoints"
  log_dir: "outputs/distill/logs"
